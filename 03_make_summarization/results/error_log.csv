video_id,trial_number,model_name,compression_rate,reason,timestamp
v6AnCELXV4Q,0,calm3,0.21236203565420878,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-19 21:46:50
v6AnCELXV4Q,1,calm3,0.14680559213273098,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-19 22:36:14
v6AnCELXV4Q,2,qwen2,0.2803345035229627,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-19 23:13:17
v6AnCELXV4Q,3,qwen2,0.34973279224012654,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-19 23:49:14
v6AnCELXV4Q,4,calm3,0.19127267288786132,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 00:45:13
v6AnCELXV4Q,5,calm3,0.2835558684167139,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 01:52:14
v6AnCELXV4Q,6,qwen2,0.2368209952651108,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 02:27:05
v6AnCELXV4Q,7,calm3,0.2777243706586128,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 03:33:47
v6AnCELXV4Q,8,gpt-oss,0.11951547789558387,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 04:37:03
v6AnCELXV4Q,9,gpt-oss,0.19138413075201122,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 05:56:47
v6AnCELXV4Q,10,qwen2,0.36048032800397883,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 06:36:07
v6AnCELXV4Q,11,qwen2,0.3796871025735363,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 07:14:05
v6AnCELXV4Q,12,qwen2,0.32869777604714945,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 07:48:33
v6AnCELXV4Q,13,qwen2,0.30589862876770213,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 08:23:16
v6AnCELXV4Q,14,qwen2,0.300200690750086,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 08:57:50
v6AnCELXV4Q,15,qwen2,0.25653074444758145,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 09:29:30
v6AnCELXV4Q,16,gpt-oss,0.3210260015521066,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 12:10:19
v6AnCELXV4Q,17,qwen2,0.24443351116247902,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 12:52:55
v6AnCELXV4Q,18,qwen2,0.3954744433821195,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 13:29:53
v6AnCELXV4Q,19,qwen2,0.22381043418720595,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 14:02:53
v6AnCELXV4Q,20,gpt-oss,0.15010475895514885,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 15:07:25
v6AnCELXV4Q,21,qwen2,0.2389158819878812,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 15:43:54
v6AnCELXV4Q,22,qwen2,0.2092387440555935,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 16:15:41
v6AnCELXV4Q,23,qwen2,0.20633943473449157,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 16:47:24
v6AnCELXV4Q,24,qwen2,0.1843925718172702,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 17:19:23
v6AnCELXV4Q,25,qwen2,0.16997032134013348,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 17:50:59
v6AnCELXV4Q,26,qwen2,0.20671462232274937,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 18:22:28
v6AnCELXV4Q,27,qwen2,0.25444269290213845,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 18:55:11
v6AnCELXV4Q,28,gpt-oss,0.11218853906253633,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 19:49:52
v6AnCELXV4Q,29,qwen2,0.21463663200559918,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 20:23:33
v6AnCELXV4Q,30,calm3,0.15701724059009192,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 21:13:17
v6AnCELXV4Q,31,qwen2,0.21302526251720125,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 21:47:11
v6AnCELXV4Q,32,qwen2,0.2176447125920723,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 22:19:16
v6AnCELXV4Q,33,qwen2,0.2575558156572407,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 22:51:52
v6AnCELXV4Q,34,qwen2,0.23385224619738912,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 23:23:12
v6AnCELXV4Q,35,qwen2,0.20316401137980172,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-20 23:56:28
v6AnCELXV4Q,36,calm3,0.1734717580290464,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-21 00:47:58
v6AnCELXV4Q,37,qwen2,0.2710307845927885,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-21 01:24:46
v6AnCELXV4Q,38,qwen2,0.13097158038647633,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-21 01:53:48
v6AnCELXV4Q,39,calm3,0.19163306321216825,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-21 02:48:05
v6AnCELXV4Q,40,qwen2,0.24002349104880683,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-21 03:23:55
v6AnCELXV4Q,41,qwen2,0.2804408285954716,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-21 03:43:19
v6AnCELXV4Q,42,qwen2,0.26492516027609003,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-21 04:15:35
v6AnCELXV4Q,43,qwen2,0.2309443350226479,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-21 04:48:38
v6AnCELXV4Q,44,qwen2,0.2689557779329298,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-21 05:22:58
v6AnCELXV4Q,45,qwen2,0.20103298853071275,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-21 05:56:16
v6AnCELXV4Q,46,gpt-oss,0.30033773487522786,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-21 08:13:15
v6AnCELXV4Q,47,calm3,0.24435456087878826,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-21 09:25:41
v6AnCELXV4Q,48,qwen2,0.28876447610832173,"Tokenization error: Input is too long, it can't be more than 49149 bytes, was 50859",2025-11-21 10:02:14
